{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TWEEPY SCRAPING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import jsonpickle\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Connection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Keys:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_key = os.getenv('CONSUMER_KEY')\n",
    "consumer_secret = os.getenv('CONSUMER_SECRET')\n",
    "access_token = os.getenv('ACCESS_TOKEN')\n",
    "access_token_secret = os.getenv('ACCESS_TOKEN_SECRET')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setup:**\n",
    "\n",
    "Application only Auth instead of the Access Token Auth allows being able to search at a rate greater than 18K tweets/15 mins. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "# auth.set_access_token(access_token, access_token_secret)\n",
    "# api = tweepy.API(auth,wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = tweepy.AppAuthHandler(consumer_key, consumer_secret)\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True,wait_on_rate_limit_notify=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save tweets to .txt:**\n",
    "\n",
    "Reference code can be found in this [link](https://bhaskarvk.github.io/2015/01/how-to-use-twitters-search-rest-api-most-effectively./#:~:text=Application%20only%20auth%20has%20higher,Auth%20using%20the%20Tweepy%20API.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_tweets_to_txt(searchQuery,maxTweets):\n",
    "    fName = '{0}-tweets.txt'.format(searchQuery)\n",
    "    sinceId = None\n",
    "    max_id = -1\n",
    "    tweetCount = 0\n",
    "\n",
    "    print(\"Downloading max {0} tweets\".format(maxTweets))\n",
    "\n",
    "    with open(fName, 'w') as f:\n",
    "        while tweetCount < maxTweets:\n",
    "            try:\n",
    "                if (max_id <= 0):\n",
    "                    if (not sinceId):\n",
    "                        new_tweets = api.search(q=searchQuery, count=100)\n",
    "                    else:\n",
    "                        new_tweets = api.search(q=searchQuery, count=100,\n",
    "                                            since_id=sinceId)\n",
    "                else:\n",
    "                    if (not sinceId):\n",
    "                        new_tweets = api.search(q=searchQuery, count=100,\n",
    "                                            max_id=str(max_id - 1))\n",
    "                    else:\n",
    "                        new_tweets = api.search(q=searchQuery, count=100,\n",
    "                                            max_id=str(max_id - 1),\n",
    "                                            since_id=sinceId)\n",
    "                if not new_tweets:\n",
    "                    print(\"No more tweets found\")\n",
    "                    break\n",
    "                for tweet in new_tweets:\n",
    "                    f.write(jsonpickle.encode(tweet._json, unpicklable=False) +'\\n')\n",
    "                tweetCount += len(new_tweets)\n",
    "                print(\"Downloaded {0} tweets\".format(tweetCount))\n",
    "                max_id = new_tweets[-1].id\n",
    "                \n",
    "            except tweepy.TweepError as e:\n",
    "            # Just exit if any error\n",
    "                print(\"some error : \" + str(e))\n",
    "                break\n",
    "\n",
    "    print (\"Downloaded {0} tweets, Saved to {1}\".format(tweetCount, fName))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following code tu save load the .txt into a json object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_name=''\n",
    "\n",
    "# with open(file_name) as file:\n",
    "#     status = [json.loads(line) for line in file]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save tweets to DataFrame:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweets_to_df(searchQuery,maxTweets,language='',geocode=[]):\n",
    "    # If results from a specific ID onwards are reqd, set since_id to that ID.\n",
    "    # else default to no lower limit, go as far back as API allows\n",
    "    sinceId = None\n",
    "\n",
    "    # If results only below a specific ID are, set max_id to that ID.\n",
    "    # else default to no upper limit, start from the most recent tweet matching the search query.\n",
    "    max_id = -1\n",
    "    tweetCount = 0\n",
    "\n",
    "    print(\"Downloading max {0} tweets\".format(maxTweets))\n",
    "\n",
    "    tweets=[]\n",
    "\n",
    "    while tweetCount < maxTweets:\n",
    "        try:\n",
    "            if (max_id <= 0):\n",
    "                if (not sinceId):\n",
    "                    new_tweets = api.search(q=searchQuery, count=100,lang=language,tweet_mode='extended')\n",
    "                else:\n",
    "                     new_tweets = api.search(q=searchQuery, count=100,lang=language,since_id=sinceId,tweet_mode='extended')\n",
    "            else:\n",
    "                if (not sinceId):\n",
    "                    new_tweets = api.search(q=searchQuery, count=100,lang=language,tweet_mode='extended',\n",
    "                                            max_id=str(max_id - 1))\n",
    "                else:\n",
    "                    new_tweets = api.search(q=searchQuery, count=100,lang=language,tweet_mode='extended',\n",
    "                                            max_id=str(max_id - 1),\n",
    "                                            since_id=sinceId)\n",
    "            if not new_tweets:\n",
    "                print(\"No more tweets found\")\n",
    "                break\n",
    "            for tweet in new_tweets:\n",
    "                tweets.append(tweet._json)\n",
    "            tweetCount += len(new_tweets)\n",
    "            print(\"Downloaded {0} tweets\".format(tweetCount))\n",
    "            max_id = new_tweets[-1].id\n",
    "        \n",
    "        except tweepy.TweepError as e:\n",
    "            # Just exit if any error\n",
    "            print(\"some error : \" + str(e))\n",
    "            break\n",
    "\n",
    "    print (\"Downloaded {0} tweets\".format(tweetCount))\n",
    "    return pd.DataFrame(tweets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping & saving to CSV 🙌🏼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set maximun tweets: 2000\n",
      "Choose query: todes\n",
      "Choose language or leave empty: es\n",
      "latitude: \n",
      "longitude: \n",
      "radius: \n"
     ]
    }
   ],
   "source": [
    "max_tweets = int(input('Set maximun tweets: '))\n",
    "query = input('Choose query: ')\n",
    "language = input('Choose language or leave empty: ')\n",
    "geocode = [input('latitude: '), input('longitude: '),input('radius: ')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading max 2000 tweets\n",
      "Downloaded 100 tweets\n",
      "Downloaded 200 tweets\n",
      "Downloaded 300 tweets\n",
      "Downloaded 400 tweets\n",
      "Downloaded 500 tweets\n",
      "Downloaded 600 tweets\n",
      "Downloaded 700 tweets\n",
      "Downloaded 800 tweets\n",
      "Downloaded 900 tweets\n",
      "Downloaded 1000 tweets\n",
      "Downloaded 1100 tweets\n",
      "Downloaded 1200 tweets\n",
      "Downloaded 1300 tweets\n",
      "Downloaded 1400 tweets\n",
      "Downloaded 1500 tweets\n",
      "Downloaded 1600 tweets\n",
      "Downloaded 1700 tweets\n",
      "Downloaded 1800 tweets\n",
      "Downloaded 1900 tweets\n",
      "Downloaded 2000 tweets\n",
      "Downloaded 2000 tweets\n"
     ]
    }
   ],
   "source": [
    "tweets = tweets_to_df(query,max_tweets,language,geocode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Change truncated tweets to full_text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['condition'] = tweets['retweeted_status'].isna()\n",
    "tweets['full_text'] = tweets.apply(lambda x: x.full_text if (x.condition == True) else x.retweeted_status['full_text'] , axis = 1)\n",
    "tweets.drop('condition', inplace=True,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.to_csv(' '.join(query.split())+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('Ironhack': conda)",
   "language": "python",
   "name": "python37664bitironhackconda2e1d8dd202d44dc690958f22a90cf36f"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
